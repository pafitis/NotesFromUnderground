{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Notes from Underground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pafitis.io\n",
    "\n",
    "This is not an exhaustive list. This is a collection of personal notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "\n",
    "\n",
    "### K-Nearest Neighbours\n",
    "\n",
    "Calculate the distance between your observation point and all other points in the training dataset. The K smallest (argmin) values of distance are a collection of the nearest neighbours and the observation is classified as the modal class in that collection. \n",
    "\n",
    "The choice for a *distance* measure depends on you, usually either:\n",
    "- Euclidian, \n",
    "- Hamming (usually for categorical data),\n",
    "- Minkowsky (p-norm; vary p depending on scenario).     \n",
    "\n",
    "\n",
    "*K-NN's require labeled data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-D Trees\n",
    "\n",
    "A quicker classification algorithm than K-nearest neighbours. It is an approximate method which means that sometimes you will make mistakes.\n",
    "\n",
    "Your data-points form a hyperspace. We select a random dimension and use the median value to split the space into two smaller sections, the *half-spaces*. We repeat this for all dimensions, ultimately dividing the whole hyperspace into a tree-like collection of *half-spaces*. \n",
    "\n",
    "When a new observation is needed to be compared, the algorithm starts by comparing each dimension singly and progresses to the appropriate *half-space* just like a decision-tree. \n",
    "\n",
    "This is faster than a K-NN but you are not guaranteed that the way the hyperspace has been divided is flawless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Alt](https://www.researchgate.net/profile/Nadhir_Ben_Halima/publication/302594789/figure/fig3/AS:360302345506820@1462914181944/Basic-KD-Tree-each-Circle-Represents-a-Feature-and-Each-Area-is-Used-to-Limit-the.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "\n",
    "### Principal Component Analysis\n",
    "\n",
    "PCA is mathematically defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.\n",
    "\n",
    "---------------------------\n",
    "\n",
    "PCA can be thought of as fitting a p-dimensional ellipsoid ([look here](www.sthda.com/sthda/RDoc/figure/factoextra/fviz_pca-principal-component-analysis-individuals-groups-factoextra-data-mining-3.png)) to the data, where each axis of the ellipsoid represents a principal component. \n",
    "\n",
    "If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only a commensurately small amount of information.\n",
    "\n",
    "To find the axes of the ellipsoid, we must first subtract the mean of each variable from the dataset to **center the data around the origin**. Then, we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. \n",
    "\n",
    "Then we must **normalize each of the orthogonal eigenvectors to become unit vectors**. Once this is done, each of the mutually orthogonal, unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. \n",
    "\n",
    "This choice of basis will transform our covariance matrix into a diagonalised form with the diagonal elements representing the variance of each axis. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues.\n",
    "\n",
    "This procedure is sensitive to the scaling of the data, and there is no consensus as to how to best scale the data to obtain optimal results.\n",
    "\n",
    "This is [fun to read](https://stats.stackexchange.com/questions/62677/pca-on-correlation-or-covariance-does-pca-on-correlation-ever-make-sense).\n",
    "\n",
    "\n",
    "### tl;dr\n",
    "\n",
    "**When would one prefer to do PCA (or factor analysis or other similar type of analysis) on correlations (i.e. on z-standardized variables) instead of doing it on covariances (i.e. on centered variables)?**\n",
    "\n",
    "    - When the variables are different units of measurement. That's clear.\n",
    "    \n",
    "    - When one wants the analysis to reflect just and only linear associations. Pearson r is not only the covariance between the uniscaled (variance=1) variables; it is suddenly the measure of the strength of linear relationship, whereas usual covariance coefficient is receptive to both linear and monotonic relationship.\n",
    "    \n",
    "    - When one wants the associations to reflect relative co-deviatedness (from the mean) rather than raw co-deviatedness. The correlation is based on distributions, their spreads, while the covariance is based on the original measurement scale. If I were to factor-analyze patients' psychopathological profiles as assesed by psychiatrists' on some clinical questionnaire consisting of Likert-type items, I'd prefer covariances. Because the professionals are not expected to distort the rating scale intrapsychically. If, on the other hand, I were to analyze the patients' self-portrates by that same questionnaire I'd probably choose correlations. Because layman's assessment is expected to be relative \"other people\", \"the majority\" \"permissible deviation\" or similar implicit das Man loupe which \"shrinks\" or \"stretches\" the rating scale for one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
